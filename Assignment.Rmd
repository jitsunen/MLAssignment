```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
library(caret)
library(C50)
library(MASS)
library(randomForest)

# Download the data
```{r, cache=TRUE, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
if( !file.exists("pml-training.csv") )
{
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, method = "curl", destfile="pml-training.csv")
}
trainData = read.csv("pml-training.csv")
if( !file.exists("pml-test.csv") )
{
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl, method = "curl", destfile="pml-test.csv")
}
testData = read.csv("pml-test.csv")
```
---
title: "Machine Learning Assignment"
author: "Jitsunen"
date: '2015-08-21'
output: html_document
---

## Introduction
Human Activity Recognition (HAR), is one of the active research areas in Machine Learning (ML) research communities. HAR research seeks to develop Machine Learning methods to develop context-aware systems. Potential applications include elderly monitoring, weigh-loss programs, digital assistants for weight lifting exerises. This assignment develops and compares three different ML algorithms: C50, Linear Discriminant Analysis (LDA), and Random Forests, to learn and predict the activity labels for the Weight lifting exercise (WLE) data set from the HAR project.

## Data Processing
### Data Set Overview
The WLE data set was generated by asking ten participants to do ten repetitions of unilateral dumbbell biceps curls in five different ways: according to the specification (class A), throwing elbows to the front (class B), lifing dumbbell halfway (class C), lowering the dumbbell halfway (class D), and throwing hips to front (class E). While the participants were doing these exercises, four sensors were recording data at periodic intervals. These sensors were located at the arm, foreram, waist, and dumbbell for each participant. 

### Pre-processing

```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
featuresSuperSet = c('roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 
 'gyros_belt_x', 'gyros_belt_y', 'gyros_belt_z', 
 'accel_belt_x', 'accel_belt_y', 'accel_belt_z', 
 'magnet_belt_x', 'magnet_belt_y', 'magnet_belt_z',
 'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm', 
 'gyros_arm_x', 'gyros_arm_y', 'gyros_arm_z', 
 'accel_arm_x', 'accel_arm_y', 'accel_arm_z', 
 'magnet_arm_x', 'magnet_arm_y', 'magnet_arm_z', 
 'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell', 'total_accel_dumbbell', 
 'gyros_dumbbell_x', 'gyros_dumbbell_y', 'gyros_dumbbell_z',
 'accel_dumbbell_x', 'accel_dumbbell_y', 'accel_dumbbell_z', 
 'magnet_dumbbell_x', 'magnet_dumbbell_y', 'magnet_dumbbell_z',
 'roll_forearm', 'pitch_forearm', 'yaw_forearm', 'total_accel_forearm', 
 'gyros_forearm_x', 'gyros_forearm_y', 'gyros_forearm_z', 
 'accel_forearm_x', 'accel_forearm_y', 'accel_forearm_z', 
 'magnet_forearm_x', 'magnet_forearm_y', 'magnet_forearm_z', 'classe')

trainData = subset(trainData, select = featuresSuperSet)

# check for missing values
lapply(is.na(train)[1:52], function(x) sum(x)) > 0

# Split train data into a train set and a validation set.
dataPartition <- createDataPartition(y = trainData$classe, p = 0.6, list = FALSE)
trainingSet <- trainData[dataPartition, ]
validationSet <- trainData[-dataPartition, ]

validationPartition <- createDataPartition(y = validationSet$classe, p = 0.5, list=FALSE)
modelComparisonSet <- validationSet[validationPartition, ]
modelPredictionSet <- validationSet[-validationPartition, ]

# Set seed.
set.seed(555)

```

The training data set consists of `r ncol(trainData)` columns and `r nrow(trainData)` rows. I couldn't find the code book for the data, so chose the columns which correspond to the sensor measurements. These columns are: roll_belt, pitch_belt, yaw_belt, total_accel_belt,  gyros_belt_x, gyros_belt_y, gyros_belt_z,  accel_belt_x, accel_belt_y, accel_belt_z,  magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm,  gyros_arm_x, gyros_arm_y, gyros_arm_z,  accel_arm_x, accel_arm_y, accel_arm_z,  magnet_arm_x, magnet_arm_y, magnet_arm_z,  roll_dumbbell, pitch_dumbbell, yaw_dumbbell, total_accel_dumbbell,  gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z,  magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm,  gyros_forearm_x, gyros_forearm_y, gyros_forearm_z,  accel_forearm_x, accel_forearm_y, accel_forearm_z,  magnet_forearm_x, magnet_forearm_y, magnet_forearm_z, classe. 

There were no missing values in the data sets.

## Fitting Models
### Data split
The data is split into a train, validation, and test set in the ratio of 60%, 20%, and 20% respectively. The train and validation data sets are used for training and testing different ML algorithms. The test data set is used for predicting the performance metrics of the selected algorithm on future data sets.

The dimensions of each data set are:

* Training set: `r dim(trainData)`.
* Validation set: `r dim(modelComparisonSet)`
* Test set: `r dim(modelPredictionSet)`

### C50 Model
```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
# C5.0 Algorithm
c50Model <- C5.0(trainingSet[, 0:52], trainingSet[, 53])
c50ValidationPredicted <- predict(c50Model, modelComparisonSet[, 0:52], type="class")
cm <- confusionMatrix(data = c50ValidationPredicted, modelComparisonSet[, 53])
```

After fitting the C50 model on training data, its performance on validation data is:
```{r, echo=FALSE, results='markup'}
cm$byClass
cm$overall
```

### LDA Model
```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
# LDA
ldaModel <- lda(classe ~ ., data = trainingSet)
ldaValidationPredicted <- predict(ldaModel, modelComparisonSet[, 0:52], type="class")
cm <- confusionMatrix(data = ldaValidationPredicted$class, modelComparisonSet[, 53])
```

After fitting the LDA model on training data, its performance on validation data is:
```{r, echo=FALSE, results='markup'}
cm$byClass
cm$overall
```

### Random Forests Model
```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
rfModel <- randomForest(classe ~ ., data=trainingSet)
rfValidationPredicted <- predict(rfModel, modelComparisonSet[, 0:52], type="class")
cm <- confusionMatrix(data = rfValidationPredicted, modelComparisonSet[, 53])
```

After fitting the Random Forests model on training data, its performance on validation data is:
```{r, echo=FALSE, results='markup'}
cm$byClass
cm$overall
```

### Best Model
From the above performance metrics we can see that C5.0 and Random Forests algorithms performed much better than LDA. C5.0 and Random Forests were close on the overall metrics, but, Random Forests was better relatively.

## Best Model Performance Prediction
The performance of Random Forests model on test set split from training set is:
```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
rfValidationPredicted <- predict(rfModel, modelPredictionSet[, 0:52], type="class")
cm <- confusionMatrix(data = rfValidationPredicted, modelPredictionSet[, 53])
```

```{r, echo=FALSE, results='markup'}
cm$byClass
cm$overall
```

The out of sample error accuracy is `r accuracy <- 100 * sum(rfValidationPredicted == modelPredictionSet[, 53])/length(rfValidationPredicted); accuracy`%. 

Out of sample error is `r 100 - accuracy`%.

## Assignment Test Data Prediction
The predicted values for the test set from the second part of the assignment, using the Random Forests algorithm are:

```{r, echo=FALSE, results='markup'}
predict(rfModel, testData, type="class")
```

## References
* [Human Activity Recognition Data Set](http://groupware.les.inf.puc-rio.br/har)
